---
title: "hyperparameter-tuning"
format: html
editor: visual
---

Lab Set Up

```{r}
library(powerjoin)
library(glue)
library(vip)
library(purrr)
library(baguette)
library(readr)
library(tidyverse)
library(tidymodels)

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

# download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
#               'data/camels_attributes_v2.0.pdf')

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
#remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')

#walk2(remote_files, local_files, download.file, quiet = TRUE)

# camels <- map(local_files, read_delim, show_col_types = FALSE) 
# 
# camels <- power_full_join(camels ,by = 'gauge_id')

camels <- map(local_files, read_delim, show_col_types = FALSE) |> 
  power_full_join(by = 'gauge_id') 


camels <- select(camels, p_mean, aridity, q_mean, gauge_lat, gauge_lon) %>% 
  drop_na()

summary(camels)
```

train & test & recipe

```{r}
#train and test split
set.seed(1001)
split <- initial_split(camels, prop = 0.8)
camels_train <- training(split)
camels_test <- testing(split)
camels_folds <- vfold_cv(camels_train, v = 10)

#recipe
rec <- recipe(q_mean ~ ., data = camels_train) |>
  step_rm(gauge_lat, gauge_lon) |>
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal()) |>
  step_scale(all_numeric_predictors()) |>
  step_center(all_numeric_predictors())

```

Build 3 models

```{r}
#| message: false

b_mod <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression")

nn_mod <- mlp(hidden_units = 10) |>
  set_engine("nnet") |>
  set_mode("regression")

rf_model <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("regression")

#Workflow
models <- list(
  rf = rf_model,
  boost = b_mod,
  nnet = nn_mod
)

rm(wf)

wf <- workflow_set(list(rec), list(b_mod, nn_mod, rf_model))%>%
  workflow_map("fit_resamples", resamples = camels_folds)

autoplot(wf)
```


The model that I think best performs is random forest model because it has the most consistent values in the autoplot.
I chose the Random Forest model. The model engine is a ranger, and the mode is regression. I think that it is performing well for this problem because of what I see in the autoplot. In rmse and rsq, the values show that this model would be better based on the fact that the values are higher than boost_tree and mlp.

Model Tuning

```{r}
library(bonsai)
rf_mod_tune <- rand_forest(trees = tune(), min_n = tune()) |> 
  set_engine("ranger") |> 
  set_mode("regression")

wf_tune <- workflow() |> 
  add_model(rf_mod_tune) |> 
  add_recipe(rec)

dials <- extract_parameter_set_dials(wf_tune) 
dials$object

my.grid <- dials |> 
  #update(trees = trees(c(50, 500))) |>
  grid_latin_hypercube(size = 20)

range(my.grid$trees)
camels_metrics = metric_set(rsq, rmse, mae)

model_params <-  tune_grid(
  wf_tune,
  resamples = camels_folds,
  grid = my.grid,
  #metrics = camels_metrics
)
```
Checking Skill of Model 
```{r}
autoplot(model_params)

show_best(model_params, metric = "rsq")
show_best(model_params, metric = "rmse")

hp_best <- select_best(model_params, metric = "rsq")

finalize <- finalize_workflow(wf_tune, hp_best)

```

```{r}
collect_metrics(model_params) |>
  filter(.metric == "rsq") |>
  arrange(desc(mean)) 
```
I see that the highest rsq values lies with the 135 tree plot. The mean is at 0.893 for that tree value. 

```{r}
final_fit <- last_fit(finalize, split, metrics = camels_metrics)

collect_predictions(final_fit) |> 
  ggplot(aes(x = .pred, y = q_mean)) + 
  geom_point() +
  geom_abline() + 
  geom_smooth(method = "rf") + 
  theme_linedraw() + 
  labs(title = "Final Fit", 
       x = "Predicted", 
       y = "Actual")
```
Finalize Model 
```{r}
final_fit <- last_fit(finalize, split, metrics = camels_metrics)
collect_metrics(final_fit)
```

Final Verification
```{r}
final_fit <- last_fit(finalize, split, metrics = camels_metrics)
```


```{r}
full_pred = fit(finalize, data = camels) |>
  augment(new_data = camels) 

full_pred <- full_pred %>%
  mutate(sq_residuals = (.pred - q_mean)^2)

ggplot(full_pred, aes(x = .pred, y = q_mean)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  theme_linedraw() +
  labs(title = "Final Fit", 
       x = "Predicted", 
       y = "Actual")
```
Bulding a Map
```{r}
final <- fit(finalize, data = camels) %>%
  augment(new_data = camels) %>%
  mutate(
  residual = .pred - q_mean,
  squared_residual = (.pred - q_mean)^2
)
```

-> Building the actual maps 
```{r}
map1 <- ggplot(data = full_pred, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = .pred)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()+
  labs(title = "Camel Gauge Locations based on q_mean Predictions",
       x = "Longitude",
       y = "Latitude",
       color = "predictions" )
```

```{r}
map2 <- ggplot(data = full_pred, aes(x = q_mean, y = .pred)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = residuals)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()+ 
  labs(title = "Camel Gauge Locations based on residual",
       x = "Longitude",
       y = "Latitude",
       color = "residuals")
```

```{r}
#combined <- map1 + map2
#print(combined)
```


#my maps have the right code (checked by jaque in class) but are not running. I am making a note that I did get maps at one point, but I changed nothing and tried to re-run R and then it stopped working 


